{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6a7bff-0d54-4fa9-8a85-d9b85b7e9fb8",
   "metadata": {},
   "source": [
    "你写的这段代码是一个完整的 **PyTorch 模型训练流程**，包括：\n",
    "\n",
    "- 定义模型 `Model()`\n",
    "- 使用优化器 `Adam`\n",
    "- 使用损失函数 `CrossEntropyLoss`\n",
    "- 使用数据加载器 `DataLoader` 加载训练集和测试集\n",
    "- 进行多个 `epoch` 的训练与验证\n",
    "- 保存每轮的 loss 和 accuracy 到列表中\n",
    "\n",
    "现在我们来重点解释你提到的两个概念：**batch size（批量大小）** 和 **epoch（轮次）**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 什么是 Batch Size（批量大小）？\n",
    "\n",
    "### 🔍 定义：\n",
    "> **Batch Size 是每次前向传播和反向传播中使用的样本数量。**\n",
    "\n",
    "### ✅ 举例说明：\n",
    "\n",
    "假设你有 1000 张图片用于训练。\n",
    "\n",
    "- 如果你设置 `batch_size=32`，那么每一轮训练中，你的模型会分批次处理这些数据：\n",
    "  - 第一个 batch：使用前 32 张图片进行训练\n",
    "  - 第二个 batch：使用接下来的 32 张图片\n",
    "  - ……\n",
    "  - 总共需要大约 1000 / 32 = 31.25 → 即 **32 个 batches**\n",
    "\n",
    "每个 batch 都会进行一次 forward + loss 计算 + backward + 参数更新。\n",
    "\n",
    "### 📌 作用：\n",
    "- 控制内存使用量（太大的 batch 可能导致显存不足）\n",
    "- 影响训练速度和收敛性\n",
    "- 小 batch 更具有“随机性”，有助于跳出局部最优；大 batch 收敛更快但可能泛化差一些\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 什么是 Epoch（训练轮次）？\n",
    "\n",
    "### 🔍 定义：\n",
    "> **Epoch 表示整个训练集被完整遍历一次的次数。**\n",
    "\n",
    "### ✅ 举例说明：\n",
    "\n",
    "还是上面的例子，1000 张图片，batch size = 32，总共约 32 个 batches。\n",
    "\n",
    "- 一个 epoch：把这 32 个 batches 全部跑完一遍，即整个训练集过了一遍模型。\n",
    "- 如果你训练 10 个 epochs，就是让模型看到训练集 10 次。\n",
    "\n",
    "### 📌 作用：\n",
    "- 控制训练时间长短\n",
    "- 多个 epoch 可以让模型更好地学习数据中的规律\n",
    "- 一般情况下，训练 loss 会随着 epoch 增加而下降，但 too many epochs 可能会导致 overfitting（过拟合）\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 你代码中的 batch size 设置如下：\n",
    "\n",
    "```python\n",
    "DL_train = DataLoader(DS_Train, batch_size=64, shuffle=True)\n",
    "DL_test = DataLoader(DS_Test, batch_size=256, shuffle=False)\n",
    "```\n",
    "\n",
    "- 训练时每个 batch 用 64 张图\n",
    "- 测试时每个 batch 用 256 张图（测试阶段可以更大，因为不需要反向传播）\n",
    "\n",
    "> 通常我们会让测试的 batch size 大于训练的 batch size，这样可以加快评估速度。\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 你设置了 50 个 epoch：\n",
    "\n",
    "```python\n",
    "epochs = 50 \n",
    "```\n",
    "\n",
    "表示你的模型会将整个训练集遍历 50 次，每次都尝试最小化损失函数，并在测试集上评估表现。\n",
    "\n",
    "---\n",
    "\n",
    "## 📉 损失和准确率记录\n",
    "\n",
    "```python\n",
    "train_loss_gpu = []\n",
    "train_acc_gpu = []\n",
    "tst_loss_gpu = []\n",
    "tst_acc_gpu = []\n",
    "```\n",
    "\n",
    "你在每个 epoch 结束后都记录下：\n",
    "- 训练损失、训练准确率\n",
    "- 测试损失、测试准确率\n",
    "\n",
    "这样你可以画出训练曲线，观察模型是否收敛、是否存在过拟合或欠拟合。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 小结：batch size vs epoch\n",
    "\n",
    "| 概念 | 含义 | 示例 | 作用 |\n",
    "|------|------|------|------|\n",
    "| **Batch Size** | 每次训练使用的样本数 | 64 张图/次 | 控制训练速度、内存占用、梯度噪声 |\n",
    "| **Epoch** | 整个训练集被训练多少遍 | 50 次 | 控制训练总次数，影响模型性能 |\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 扩展建议\n",
    "\n",
    "### 1. 如何选择 batch size？\n",
    "- GPU 显存越大，可以选越大的 batch size（如 256、512）\n",
    "- 小 batch（如 32、64）更适用于小数据集或容易过拟合的情况\n",
    "- 推荐从 `64` 开始尝试，再根据效果调整\n",
    "\n",
    "### 2. 如何选择 epoch 数量？\n",
    "- 简单任务（如 MNIST）可能 5~10 轮就收敛了\n",
    "- 复杂任务（如图像分类 CIFAR-10）可能需要 50~100 轮\n",
    "- 可配合早停机制（Early Stopping）避免过拟合\n",
    "\n",
    "---\n",
    "\n",
    "## 🎁 举个类比帮助理解：\n",
    "\n",
    "想象你要背单词考试：\n",
    "\n",
    "- **Batch Size**：你每次只背 10 个单词（一个 batch），然后去测试一下自己记住了几个。\n",
    "- **Epoch**：你把整本单词书背完了就算一个 epoch。\n",
    "- 你反复背很多轮（多 epoch），并且每次背不同数量的单词（不同 batch size），最终目标是考试考高分 😄\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想了解如何可视化训练过程、如何调整学习率、或者 Early Stopping 的实现方法，我也可以继续帮你写代码 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2686e48e-06fe-4fcc-a1af-f2486a6a23a5",
   "metadata": {},
   "source": [
    "是的，你说得完全正确！在这段代码中：\n",
    "\n",
    "```python\n",
    "for x, y in Train_DL:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    y_pred = Model_m(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "```\n",
    "\n",
    "你是在 **逐批次（batch）地训练模型**。也就是说，每次循环传入的是一个 **batch 的数据**。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 详细解释：这段代码在做什么？\n",
    "\n",
    "### 🔁 `for x, y in Train_DL:`\n",
    "\n",
    "- `Train_DL` 是一个 PyTorch 的 `DataLoader` 对象。\n",
    "- 每次迭代会从数据集中取出一个 batch 的数据。\n",
    "- `x` 是输入特征（如图像）\n",
    "- `y` 是对应的标签（如类别）\n",
    "\n",
    "例如：\n",
    "```python\n",
    "x.shape  # 可能是 [64, 1, 28, 28] 表示 batch_size=64，图像是 28x28 的灰度图\n",
    "y.shape  # 可能是 [64] 表示这 64 张图片对应的真实类别\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 所以，“一个 batch”到底是什么意思？\n",
    "\n",
    "| 名词 | 含义 |\n",
    "|------|------|\n",
    "| **Batch** | 一批数据，通常是一个 mini-batch，即一小部分训练集 |\n",
    "| **Batch Size** | 每个 batch 中包含多少个样本（比如上面例子中的 64） |\n",
    "\n",
    "> 在你的代码中，每个 batch 包含 64 个样本（因为你在创建 DataLoader 时设置了 `batch_size=64`），所以每次循环处理的是 64 张图片和它们的标签。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 举个类比帮助理解：\n",
    "\n",
    "想象你有一本书，里面有 60000 页（像 MNIST 数据集）：\n",
    "\n",
    "- 每天你只读 **64 页**（这就是一个 batch）\n",
    "- 把整本书读完一遍就是 **一个 epoch**\n",
    "- 如果你读了 50 遍（训练 50 个 epoch），那总共就读了 `60000 × 50 = 3,000,000` 页\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 为什么使用 mini-batch 而不是一次性训练全部数据？\n",
    "\n",
    "### ✅ 好处一：节省内存\n",
    "- 一次性加载所有数据（如 6w 张图片）可能超出 GPU 显存限制\n",
    "- 使用 mini-batch（如 64、128、256）可以控制显存使用量\n",
    "\n",
    "### ✅ 好处二：加快训练速度\n",
    "- 小批量更新参数更频繁，收敛更快\n",
    "- 大 batch 虽然单次计算效率高，但更新次数少，效果反而不一定更好\n",
    "\n",
    "### ✅ 好处三：引入随机性，防止过拟合\n",
    "- 每次用不同的小批量数据训练，有助于模型泛化能力提升\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 总结一句话：\n",
    "\n",
    "> 是的，在 `for x, y in Train_DL:` 这个循环中，每次传入的是一个 **mini-batch 的数据**，也就是一组样本（如 64 张图片 + 它们的标签）。整个训练过程就是在不断对这些 batch 进行前向传播、反向传播、参数更新，直到遍历完整个数据集一轮（一个 epoch）。\n",
    "\n",
    "---\n",
    "\n",
    "如果你还想了解：\n",
    "\n",
    "- 如何查看每个 epoch 实际有多少个 batch？\n",
    "- 如何可视化训练过程？\n",
    "- 如何动态调整学习率？\n",
    "- 如何保存最佳模型？\n",
    "\n",
    "我也可以继续帮你写代码 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be52edb4-6e07-4954-8cb8-91aa1ac64ab8",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "batch = 64\n",
    "no_of_batches = len(data)//batch\n",
    "epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_lst = []\n",
    "for epoch in range(epochs):\n",
    "    for i in range(no_of_batches): \n",
    "        start = i*batch\n",
    "        end = start+batch\n",
    "        x = X[start:end].to(device)\n",
    "        y = Y[start:end].to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward() \n",
    "        optim.step()\n",
    "    #我们也可以在这里写计算损失值,这样在每个epoch之后都能看到损失的下降\n",
    "    # 注意这一步不需要梯度计算,所以要写到无torch梯度计算里\n",
    "    with torch.no_grad():\n",
    "        loss_ = loss_fn(model(X.to(device)),Y.to(device)).data.item()\n",
    "        loss_lst.append(loss_)\n",
    "        print(f'轮次>>:{epoch}损失:{loss_}')\n",
    "print('训练结束')\n",
    "print(loss_lst)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d9e63-4d50-4092-a130-ddc79355da05",
   "metadata": {},
   "source": [
    "例如在之前没学习dataloader时候,我们还需要手动处理批次的索引\n",
    "需要指定一批数据的大小batch \n",
    "然后让数据总长度整除batch得到batches批次的大小  \n",
    "这之后进行遍历 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
