{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d5e40b-c407-42f1-b803-d2a015f6dca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorrt_bindings.tensorrt.IBuilderConfig' object has no attribute 'max_workspace_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m engine_path\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 转换模型（替换为你的 ONNX 路径）\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mbuild_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpet_detection_model.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpet_detection.engine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36mbuild_engine\u001b[1;34m(onnx_path, engine_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 配置构建参数\u001b[39;00m\n\u001b[0;32m     15\u001b[0m config \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mcreate_builder_config()\n\u001b[1;32m---> 16\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_workspace_size\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m30\u001b[39m  \u001b[38;5;66;03m# 1GB 工作空间\u001b[39;00m\n\u001b[0;32m     17\u001b[0m config\u001b[38;5;241m.\u001b[39mset_flag(trt\u001b[38;5;241m.\u001b[39mBuilderFlag\u001b[38;5;241m.\u001b[39mFP16)  \u001b[38;5;66;03m# 启用 FP16 加速（需 GPU 支持）\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 构建并保存引擎\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorrt_bindings.tensorrt.IBuilderConfig' object has no attribute 'max_workspace_size'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        \n",
    "        # 解析 ONNX 模型\n",
    "        with open(onnx_path, 'rb') as f:\n",
    "            parser.parse(f.read())\n",
    "        \n",
    "        # 配置构建参数\n",
    "        config = builder.create_builder_config()\n",
    "        config.max_workspace_size = 1 << 30  # 1GB 工作空间\n",
    "        config.set_flag(trt.BuilderFlag.FP16)  # 启用 FP16 加速（需 GPU 支持）\n",
    "        \n",
    "        # 构建并保存引擎\n",
    "        engine = builder.build_engine(network, config)\n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(engine.serialize())\n",
    "    return engine_path\n",
    "\n",
    "# 转换模型（替换为你的 ONNX 路径）\n",
    "build_engine(\"pet_detection_model.onnx\", \"pet_detection.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7013a256-c76a-4dce-9905-e5459bd96545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "引擎构建失败！\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_path, engine_path):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        \n",
    "        # 解析 ONNX 模型\n",
    "        with open(onnx_path, 'rb') as f:\n",
    "            if not parser.parse(f.read()):\n",
    "                print(\"ONNX 解析失败，错误信息：\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "        \n",
    "        # 配置构建参数\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB\n",
    "        \n",
    "        # 启用 FP16 加速（需 GPU 支持）\n",
    "        if builder.platform_has_fast_fp16:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        \n",
    "        # 构建并保存引擎（适配新版本 API）\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            print(\"引擎构建失败！\")\n",
    "            return None\n",
    "            \n",
    "        with open(engine_path, 'wb') as f:\n",
    "            f.write(serialized_engine)\n",
    "        return engine_path\n",
    "\n",
    "# 转换模型\n",
    "build_engine(\"pet_detection_model.onnx\", \"pet_detection.engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67769c95-ff09-4bfb-a0b6-be1af0b53729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 正在解析 ONNX 模型: pet_detection_model.onnx\n",
      "✅ ONNX 解析成功，网络层数: 274\n",
      "🔄 启用 FP16 精度\n",
      "🔄 正在构建 TensorRT 引擎...\n",
      "❌ 引擎构建失败！\n",
      "  - 成功解析网络，层数: 274\n",
      "  - 最后一层: /Squeeze_3, 类型: LayerType.SQUEEZE\n",
      "  - 所有输出均已连接\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)  # 使用 ERROR 级别减少冗余日志\n",
    "\n",
    "def build_engine(onnx_path, engine_path, fp16=True, workspace_size=2):\n",
    "    \"\"\"构建 TensorRT 引擎\n",
    "    \n",
    "    Args:\n",
    "        onnx_path: ONNX 模型路径\n",
    "        engine_path: 保存引擎的路径\n",
    "        fp16: 是否启用 FP16 精度\n",
    "        workspace_size: 工作空间大小（GB）\n",
    "    \"\"\"\n",
    "    # 如果引擎已存在，直接返回\n",
    "    if os.path.exists(engine_path):\n",
    "        print(f\"✅ 引擎已存在: {engine_path}\")\n",
    "        return engine_path\n",
    "        \n",
    "    # 创建构建器和网络\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "    \n",
    "    # 解析 ONNX 模型\n",
    "    print(f\"🔄 正在解析 ONNX 模型: {onnx_path}\")\n",
    "    with open(onnx_path, 'rb') as f:\n",
    "        if not parser.parse(f.read()):\n",
    "            print(\"❌ ONNX 解析失败，错误信息:\")\n",
    "            for error in range(parser.num_errors):\n",
    "                print(f\"  - {parser.get_error(error)}\")\n",
    "            return None\n",
    "            \n",
    "    print(f\"✅ ONNX 解析成功，网络层数: {network.num_layers}\")\n",
    "    \n",
    "    # 配置构建参数\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_size << 30)\n",
    "    \n",
    "    # 启用 FP16（如果支持）\n",
    "    if fp16 and builder.platform_has_fast_fp16:\n",
    "        print(\"🔄 启用 FP16 精度\")\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    else:\n",
    "        print(\"🔄 使用 FP32 精度\")\n",
    "    \n",
    "    # 构建并保存引擎\n",
    "    print(\"🔄 正在构建 TensorRT 引擎...\")\n",
    "    serialized_engine = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    if serialized_engine is None:\n",
    "        print(\"❌ 引擎构建失败！\")\n",
    "        \n",
    "        # 输出详细错误信息\n",
    "        if network.num_layers == 0:\n",
    "            print(\"  - 网络层数为0，ONNX解析可能失败\")\n",
    "        else:\n",
    "            print(f\"  - 成功解析网络，层数: {network.num_layers}\")\n",
    "            \n",
    "        # 检查最后一层\n",
    "        last_layer = network.get_layer(network.num_layers - 1)\n",
    "        print(f\"  - 最后一层: {last_layer.name}, 类型: {last_layer.type}\")\n",
    "        \n",
    "        # 检查未连接的输出\n",
    "        unconnected_outputs = False\n",
    "        for i in range(network.num_layers):\n",
    "            layer = network.get_layer(i)\n",
    "            for j in range(layer.num_outputs):\n",
    "                if not layer.get_output(j):\n",
    "                    print(f\"  - ❌ 第 {i} 层的第 {j} 个输出未连接\")\n",
    "                    unconnected_outputs = True\n",
    "                    \n",
    "        if not unconnected_outputs:\n",
    "            print(\"  - 所有输出均已连接\")\n",
    "            \n",
    "        return None\n",
    "        \n",
    "    # 保存引擎\n",
    "    with open(engine_path, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "        \n",
    "    print(f\"✅ 引擎已成功保存到: {engine_path}\")\n",
    "    return engine_path\n",
    "\n",
    "# 转换模型\n",
    "build_engine(\n",
    "    onnx_path=\"pet_detection_model.onnx\",\n",
    "    engine_path=\"pet_detection.engine\",\n",
    "    fp16=True,\n",
    "    workspace_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48eae08c-de69-4fd5-9fab-a461da8fbe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 正在修复 ONNX 模型中的 Squeeze 节点: pet_detection_model.onnx\n",
      "  - 找到 Squeeze 节点: /Squeeze\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m original_onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpet_detection_model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# 修复 ONNX 模型\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m fixed_onnx_path \u001b[38;5;241m=\u001b[39m \u001b[43mfix_onnx_squeeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_onnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_onnx_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# 构建引擎\u001b[39;00m\n\u001b[0;32m    144\u001b[0m build_engine(\n\u001b[0;32m    145\u001b[0m     onnx_path\u001b[38;5;241m=\u001b[39mfixed_onnx_path,\n\u001b[0;32m    146\u001b[0m     engine_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpet_detection.engine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    148\u001b[0m     workspace_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    149\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mfix_onnx_squeeze\u001b[1;34m(onnx_path, output_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 计算新形状（移除维度为1的轴）\u001b[39;00m\n\u001b[0;32m     28\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_tensor\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m         new_shape\u001b[38;5;241m.\u001b[39mappend(dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "\n",
    "def fix_onnx_squeeze(onnx_path, output_path):\n",
    "    \"\"\"修复 ONNX 模型中的 Squeeze 节点\"\"\"\n",
    "    print(f\"🔄 正在修复 ONNX 模型中的 Squeeze 节点: {onnx_path}\")\n",
    "    \n",
    "    # 加载 ONNX 模型\n",
    "    graph = gs.import_onnx(onnx.load(onnx_path))\n",
    "    \n",
    "    # 查找并替换 Squeeze 节点\n",
    "    squeeze_count = 0\n",
    "    for node in graph.nodes:\n",
    "        if node.op == \"Squeeze\":\n",
    "            # 记录 Squeeze 节点\n",
    "            squeeze_count += 1\n",
    "            print(f\"  - 找到 Squeeze 节点: {node.name}\")\n",
    "            \n",
    "            # 创建新的 Reshape 节点替代 Squeeze\n",
    "            input_tensor = node.inputs[0]\n",
    "            output_tensor = node.outputs[0]\n",
    "            \n",
    "            # 计算新形状（移除维度为1的轴）\n",
    "            new_shape = []\n",
    "            for i, dim in enumerate(input_tensor.shape):\n",
    "                if dim != 1 or (i in node.attrs.get(\"axes\", []) if \"axes\" in node.attrs else True):\n",
    "                    new_shape.append(dim)\n",
    "            \n",
    "            # 创建形状张量\n",
    "            shape_tensor = gs.Constant(name=f\"{node.name}_shape\", values=np.array(new_shape, dtype=np.int64))\n",
    "            \n",
    "            # 创建 Reshape 节点\n",
    "            reshape_node = gs.Node(\n",
    "                op=\"Reshape\",\n",
    "                name=f\"{node.name}_reshape\",\n",
    "                inputs=[input_tensor, shape_tensor],\n",
    "                outputs=[output_tensor]\n",
    "            )\n",
    "            \n",
    "            # 添加新节点并移除原 Squeeze 节点\n",
    "            graph.nodes.append(reshape_node)\n",
    "            node.outputs = []\n",
    "    \n",
    "    if squeeze_count > 0:\n",
    "        print(f\"✅ 成功替换 {squeeze_count} 个 Squeeze 节点\")\n",
    "        # 清理并保存修改后的模型\n",
    "        graph.cleanup().toposort()\n",
    "        onnx.save(gs.export_onnx(graph), output_path)\n",
    "        return output_path\n",
    "    else:\n",
    "        print(\"✅ 未发现需要修复的 Squeeze 节点\")\n",
    "        return onnx_path\n",
    "\n",
    "def build_engine(onnx_path, engine_path, fp16=True, workspace_size=2):\n",
    "    \"\"\"构建 TensorRT 引擎\"\"\"\n",
    "    # 如果引擎已存在，直接返回\n",
    "    if os.path.exists(engine_path):\n",
    "        print(f\"✅ 引擎已存在: {engine_path}\")\n",
    "        return engine_path\n",
    "        \n",
    "    # 创建构建器和网络\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "    \n",
    "    # 解析 ONNX 模型\n",
    "    print(f\"🔄 正在解析 ONNX 模型: {onnx_path}\")\n",
    "    with open(onnx_path, 'rb') as f:\n",
    "        if not parser.parse(f.read()):\n",
    "            print(\"❌ ONNX 解析失败，错误信息:\")\n",
    "            for error in range(parser.num_errors):\n",
    "                print(f\"  - {parser.get_error(error)}\")\n",
    "            return None\n",
    "            \n",
    "    print(f\"✅ ONNX 解析成功，网络层数: {network.num_layers}\")\n",
    "    \n",
    "    # 配置构建参数\n",
    "    config = builder.create_builder_config()\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_size << 30)\n",
    "    \n",
    "    # 启用 FP16（如果支持）\n",
    "    if fp16 and builder.platform_has_fast_fp16:\n",
    "        print(\"🔄 启用 FP16 精度\")\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "        \n",
    "        # 对所有 Squeeze 层强制使用 FP32\n",
    "        for i in range(network.num_layers):\n",
    "            layer = network.get_layer(i)\n",
    "            if layer.type == trt.LayerType.SQUEEZE:\n",
    "                print(f\"  - 对 Squeeze 层 {layer.name} 使用 FP32 精度\")\n",
    "                layer.precision = trt.DataType.FLOAT\n",
    "                layer.set_output_type(0, trt.DataType.FLOAT)\n",
    "    \n",
    "    # 构建并保存引擎\n",
    "    print(\"🔄 正在构建 TensorRT 引擎...\")\n",
    "    serialized_engine = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    if serialized_engine is None:\n",
    "        print(\"❌ 引擎构建失败！\")\n",
    "        \n",
    "        # 输出详细错误信息\n",
    "        if network.num_layers == 0:\n",
    "            print(\"  - 网络层数为0，ONNX解析可能失败\")\n",
    "        else:\n",
    "            print(f\"  - 成功解析网络，层数: {network.num_layers}\")\n",
    "            \n",
    "        # 检查最后一层\n",
    "        last_layer = network.get_layer(network.num_layers - 1)\n",
    "        print(f\"  - 最后一层: {last_layer.name}, 类型: {last_layer.type}\")\n",
    "        \n",
    "        # 检查未连接的输出\n",
    "        unconnected_outputs = False\n",
    "        for i in range(network.num_layers):\n",
    "            layer = network.get_layer(i)\n",
    "            for j in range(layer.num_outputs):\n",
    "                if not layer.get_output(j):\n",
    "                    print(f\"  - ❌ 第 {i} 层的第 {j} 个输出未连接\")\n",
    "                    unconnected_outputs = True\n",
    "                    \n",
    "        if not unconnected_outputs:\n",
    "            print(\"  - 所有输出均已连接\")\n",
    "            \n",
    "        return None\n",
    "        \n",
    "    # 保存引擎\n",
    "    with open(engine_path, 'wb') as f:\n",
    "        f.write(serialized_engine)\n",
    "        \n",
    "    print(f\"✅ 引擎已成功保存到: {engine_path}\")\n",
    "    return engine_path\n",
    "\n",
    "# 主流程\n",
    "fixed_onnx_path = \"pet_detection_model_fixed.onnx\"\n",
    "original_onnx_path = \"pet_detection_model.onnx\"\n",
    "\n",
    "# 修复 ONNX 模型\n",
    "fixed_onnx_path = fix_onnx_squeeze(original_onnx_path, fixed_onnx_path)\n",
    "\n",
    "# 构建引擎\n",
    "build_engine(\n",
    "    onnx_path=fixed_onnx_path,\n",
    "    engine_path=\"pet_detection.engine\",\n",
    "    fp16=True,\n",
    "    workspace_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977df068-53a3-4e04-8fed-b57dc0dbbef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
