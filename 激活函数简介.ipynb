{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71ddaa0-f15d-420f-9965-67fc169f9dae",
   "metadata": {},
   "source": [
    "在 PyTorch 中，**激活函数（Activation Function）** 是神经网络中非常重要的组成部分。它们的主要作用是 **引入非线性（non-linearity）**，使得神经网络可以学习和拟合复杂的模式。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 为什么需要激活函数？\n",
    "\n",
    "神经网络的基本结构是多个线性变换的叠加：\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "如果你只使用线性变换，不管多少层，最终的结果仍然是输入的一个线性组合。也就是说：\n",
    "\n",
    "> 多个线性层堆叠 ≈ 单个线性层\n",
    "\n",
    "这会极大限制模型的表达能力。\n",
    "\n",
    "所以，我们通过在每一层之间加入**非线性激活函数**，让神经网络具备强大的建模能力，能拟合任意复杂函数（万能近似定理 Universal Approximation Theorem）。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 常见的激活函数（PyTorch 实现）\n",
    "\n",
    "以下是一些常用的激活函数及其在 PyTorch 中的调用方式：\n",
    "\n",
    "### 1. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "- 数学公式：$ \\text{ReLU}(x) = \\max(0, x) $\n",
    "- 特点：简单高效，防止梯度消失问题\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "output = F.relu(x)\n",
    "print(output)  # tensor([0., 0., 0., 1., 2.])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sigmoid**\n",
    "\n",
    "- 公式：$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- 输出范围：(0, 1)，适合二分类输出层或门控机制（如 LSTM）\n",
    "\n",
    "```python\n",
    "output = torch.sigmoid(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "- 输出范围：(-1, 1)，比 Sigmoid 更中心对称，收敛更快\n",
    "\n",
    "```python\n",
    "output = torch.tanh(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Softmax**\n",
    "\n",
    "- 输入一个向量，输出每个元素的概率分布（总和为1）\n",
    "- 常用于多分类任务的输出层\n",
    "\n",
    "```python\n",
    "x = torch.randn(3)\n",
    "output = F.softmax(x, dim=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Leaky ReLU**\n",
    "\n",
    "- ReLU 的改进版，解决“神经元死亡”问题\n",
    "- $ \\text{LeakyReLU}(x) = \\max(0.01x, x) $\n",
    "\n",
    "```python\n",
    "output = F.leaky_relu(x, negative_slope=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 在神经网络中的使用位置\n",
    "\n",
    "激活函数通常放在每一层的输出之后、下一层输入之前：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # 加上 ReLU 激活函数\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 总结：激活函数的作用\n",
    "\n",
    "| 作用 | 描述 |\n",
    "|------|------|\n",
    "| 引入非线性 | 让神经网络能够学习复杂函数 |\n",
    "| 控制输出范围 | 如 sigmoid 控制在 [0,1] 表示概率 |\n",
    "| 提高训练效率 | 合适的激活函数可以缓解梯度消失/爆炸问题 |\n",
    "\n",
    "---\n",
    "\n",
    "如果你感兴趣，我可以给你画出这些激活函数的图像，或者讲解它们在实际神经网络（如 CNN、RNN）中的应用场景。是否想继续深入？ 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fde9b-7fdc-48d6-8447-d50d669cb354",
   "metadata": {},
   "source": [
    "## 📌什么时候需要激活函数什么时候不需要激活函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57e930-fb57-444b-acb1-34eef61c1ed0",
   "metadata": {},
   "source": [
    "在深度学习中，激活函数（Activation Function）被用来为神经网络引入非线性因素，使得模型能够学习到更复杂的模式。然而，并不是所有层都需要激活函数，具体是否需要使用激活函数取决于层的功能和预期的输出类型。以下是一些指导原则，帮助理解何时以及为何需要或不需要激活函数。\n",
    "\n",
    "### 需要激活函数的情况\n",
    "\n",
    "1. **隐藏层**：几乎所有的隐藏层都需要激活函数来引入非线性。如果没有激活函数或者使用的是线性激活函数，无论网络有多少层，其表达能力都不会超过单层感知机的能力，因为多个线性变换的组合仍然是线性的。\n",
    "   - **常用激活函数**包括ReLU、tanh、sigmoid等。例如，ReLU(Rectified Linear Unit)因其简单高效而在现代神经网络中广泛应用。\n",
    "\n",
    "2. **多分类问题的输出层**：当处理多分类问题时，通常在输出层使用softmax激活函数。Softmax可以将原始预测值转化为概率分布，方便我们解释每个类别的预测概率。\n",
    "   \n",
    "3. **二分类问题的输出层**：对于二分类任务，输出层一般包含一个神经元，并使用sigmoid激活函数，该函数的输出范围在0到1之间，可视为属于正类的概率。\n",
    "\n",
    "### 不需要激活函数的情况\n",
    "\n",
    "1. **回归问题的输出层**：当你正在解决一个回归问题（如预测房价、温度等连续值），通常不需要在输出层使用激活函数。这是因为你需要直接输出未受限的预测值，而不是限制在某个特定范围内的值。在这种情况下，输出层通常只是一个线性单元（即没有激活函数）。\n",
    "   \n",
    "2. **某些特殊架构**：在一些特殊的网络架构或层中，可能不需要激活函数。例如，在自动编码器中的编码部分的最后一层有时不使用激活函数，以确保能够学习到数据的有效表示而不限制其范围。\n",
    "\n",
    "3. **批归一化层之后**：在使用批归一化（Batch Normalization）的情况下，由于批归一化本身具有一定的平滑效果，有时会省略激活函数，特别是在接近输入层的位置。不过，实践中常见做法是在批归一化后仍使用ReLU激活函数来增加非线性。\n",
    "\n",
    "总的来说，激活函数的选择很大程度上依赖于你所面对的具体问题以及你希望从网络中得到什么样的输出。正确地应用激活函数是构建有效神经网络模型的重要一步。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
